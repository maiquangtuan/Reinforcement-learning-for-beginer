{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Frozen-lake using Monte Carlo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTR+MTmZxHQKAMa5rm6E4c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maiquangtuan/Reinforcement-learning-for-beginer/blob/main/Frozen_lake_using_Monte_Carlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQxT4OLs1or"
      },
      "source": [
        "We already know that Dynamic programming required the transition model of the environment to find the optimal policy. In reality, we can not model the complex environment. Monte Carlo is a method to estimate the value by sampling the environment. Then the structure to fins the optimal policy is still the same with dynamic programming when we have the value function to evaluate the policy and improve them by choosing the best astion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWwjiQjftu8a"
      },
      "source": [
        "This notebook will show you how we can build an agent learn to move from a beginning position to the target point. The environment we will use is Frozen Lake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcruu92attCx"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import operator\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "import random\n",
        "import itertools\n",
        "import tqdm\n",
        "\n",
        "tqdm.monitor_interval = 0"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVcHmT3cuLVi"
      },
      "source": [
        "#first we define a random policy function, this kind of policy will ensure that our agent will visit all the possible (state, action) pair\n",
        "def create_random_policy(env):\n",
        "     policy = {}\n",
        "     for key in range(0, env.observation_space.n):\n",
        "          current_end = 0\n",
        "          p = {}\n",
        "          for action in range(0, env.action_space.n):\n",
        "               p[action] = 1 / env.action_space.n\n",
        "          policy[key] = p\n",
        "     return policy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux398S_Ounb7"
      },
      "source": [
        "#when we have a defined policy, the next step is define a mapping dictionary from state to action, given the environment and policy\n",
        "def create_state_action_dictionary(env, policy):\n",
        "    Q = {}\n",
        "    for key in policy.keys():\n",
        "         Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n",
        "    return Q"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efJpYSQ6vWAp"
      },
      "source": [
        "Then we define function to make agent play this game until it reach the target point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv3kQ3BdvAlo"
      },
      "source": [
        "def run_game(env, policy, display=True):\n",
        "     env.reset()\n",
        "     episode = []\n",
        "     finished = False\n",
        "\n",
        "     while not finished:\n",
        "               env.render()\n",
        "               sleep(1)\n",
        "\n",
        "          timestep = []\n",
        "          timestep.append(s)\n",
        "          n = random.uniform(0, sum(policy[s].values()))\n",
        "          top_range = 0\n",
        "          for prob in policy[s].items():\n",
        "                top_range += prob[1]\n",
        "                if n < top_range:\n",
        "                      action = prob[0]\n",
        "                      break \n",
        "          state, reward, finished, info = env.step(action)\n",
        "          timestep.append(action)\n",
        "          timestep.append(reward)\n",
        "\n",
        "          episode.append(timestep)\n",
        "\n",
        "     if display:\n",
        "          clear_output(True)\n",
        "          env.render()\n",
        "          sleep(1)\n",
        "     return episode"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47BsXCJjvqlm"
      },
      "source": [
        "#This function is evaluate the policy by counting the number of time the agent win the game if we give it 100 time to play\n",
        "def test_policy(policy, env):\n",
        "      wins = 0\n",
        "      r = 100\n",
        "      for i in range(r):\n",
        "            w = run_game(env, policy, display=False)[-1][-1]\n",
        "            if w == 1:\n",
        "                  wins += 1\n",
        "      return wins / r"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQnou41PwXrj"
      },
      "source": [
        "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
        "    if not policy:\n",
        "        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    \n",
        "    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n",
        "    returns = {} # 3.\n",
        "    \n",
        "    for _ in range(episodes): # Looping through episodes\n",
        "        G = 0 # Store cumulative reward in G (initialized at 0)\n",
        "        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively \n",
        "        \n",
        "        # for loop through reversed indices of episode array. \n",
        "        # The logic behind it being reversed is that the eventual reward would be at the end. \n",
        "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
        "        \n",
        "        for i in reversed(range(0, len(episode))):   \n",
        "            s_t, a_t, r_t = episode[i] \n",
        "            state_action = (s_t, a_t)\n",
        "            G += r_t # Increment total reward by reward on current timestep\n",
        "            \n",
        "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # \n",
        "                if returns.get(state_action):\n",
        "                    returns[state_action].append(G)\n",
        "                else:\n",
        "                    returns[state_action] = [G]   \n",
        "                    \n",
        "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n",
        "                \n",
        "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n",
        "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
        "                max_Q = random.choice(indices)\n",
        "                \n",
        "                A_star = max_Q # 14.\n",
        "                \n",
        "                for a in policy[s_t].items(): # Update action probability for s_t in policy\n",
        "                    if a[0] == A_star:\n",
        "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
        "                    else:\n",
        "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
        "\n",
        "    return policy"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weGot-7KwrL3",
        "outputId": "7067a9dd-f1e0-4a23-b3df-6eec2190a2fb"
      },
      "source": [
        "env = gym.make('FrozenLake8x8-v0')\n",
        "policy = monte_carlo_e_soft(env = env, episodes = 10000)\n",
        "test_policy(policy, env)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}